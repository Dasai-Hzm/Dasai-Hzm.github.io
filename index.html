<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
<script>
MathJax = {
  tex: {
    inlineMath: [['$','$'], ['\\(','\\)']],  // 配置行内公式分隔符
    displayMath: [['$$','$$'], ['\\[','\\]']] // 配置行间公式分隔符
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // 避免处理代码块
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto Serif Simplified Chinese:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"dasai-hzm.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="HzmSailor&#39;s Learning Blog">
<meta property="og:url" content="https://dasai-hzm.github.io/index.html">
<meta property="og:site_name" content="HzmSailor&#39;s Learning Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="HzmSailor">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://dasai-hzm.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>HzmSailor's Learning Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">HzmSailor's Learning Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">HzmSailor 的技术博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/03/21/PointNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/21/PointNet/" class="post-title-link" itemprop="url">3D Vision: 3D CNN && PointNet</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-21 22:48:16" itemprop="dateCreated datePublished" datetime="2025-03-21T22:48:16+08:00">2025-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-01 13:40:09" itemprop="dateModified" datetime="2025-04-01T13:40:09+08:00">2025-04-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="3D-Vision-数据表征方法"><a href="#3D-Vision-数据表征方法" class="headerlink" title="3D Vision 数据表征方法"></a>3D Vision 数据表征方法</h1><h2 id="Voxel"><a href="#Voxel" class="headerlink" title="Voxel"></a>Voxel</h2><p>Voxel (译为体素)，是一种结构化的数据表征方法，就是把空间划为网格，每个网格有占据/空置两种状态，如果是占据状态，则可以用 RGB 三通道来表征它的颜色。当然，一个点除了用公式写出：</p>
<p>[<br>x_i = (x, y, z, o, r, g, b, …)<br>]</p>
<p>其中，$o$ 可取 0 或 1，表示是否占据。一些情况下，也可以取 0 至 1 中的某些数，表示占据的概率。一个例子如下图：</p>
<p><img src="image-419.png" alt="alt text"></p>
<p>Voxel 的好处是结构化，形状规则，坏处是内存开销较大，分辨率恒定（在一些精细结构上，恒定的分辨率就显得不够用了）。当然，为了解决分辨率的问题，还有一种 Octree (八叉树结构), 采用树结构进行存储。一个空间被分为 8 份（8 个立方体），如果这一份里有信息，就继续细分，如果这一份里没有信息，就不再分。这样有助于节约存储空间，实现动态的分辨率。如下图：<br><img src="image-418.png" alt="alt text"></p>
<h2 id="Mesh"><a href="#Mesh" class="headerlink" title="Mesh"></a>Mesh</h2><p>Mesh 也是一种结构化存储三维数据的方法。它由顶点、边和面构成。它首先存储一个顶点列表：$x_i = (x, y, z)$，再存储一个面索引列表：$f_s = (x_i, x_j, x_k,…)$. Mesh 可以按照面的形状分类，有三角形 (Triangle Mesh)、四边形 (Quad Mesh)、不定形 (Polygon Mesh) 等等。</p>
<h1 id="3D-CNN"><a href="#3D-CNN" class="headerlink" title="3D CNN"></a>3D CNN</h1><p>3D CNN 就是把二维的卷积神经网络拓展到三维，最早的 3D CNN 被用于处理时空序列信号（如视频，见：Tran, Du, et al. “Learning spatiotemporal features with 3d convolutional networks.” Proceedings of the IEEE international conference on computer vision. 2015.），当然，3D CNN 也可用于处理 3D Vision. </p>
<p>下面介绍这篇文章中 3D CNN 的结构：与二维卷积类似，每一个卷积层仍有一个卷积核，在空间上尺寸为 3*3*3，通道数为 3 ，每次移动的 stride 为 1（在三个维度上的移动 stride 一致），卷积核的个数在图中列出。</p>
<p>每一个卷积层的后面都紧跟着一个 max pooling（PointNet也有），就是在一个 pooling kernel 的范围内，选择最大的数值以代表整个 pooling kernel. Pooling 是 CNN 的常见操作，有助于降低计算复杂度、降噪、提取到更高级/抽象的特征。当然，适不适合 pooling 要根据具体问题选择，比如 AlphaGo 在处理围棋的棋局信息时，显然就不适合用 pooling. Max pooling 可以参考下图。<br><img src="image-5.png" alt="alt text"></p>
<p>最后先要把三维特征图展平为一维，然后经过两个 FC 网络，目的是把图像的特征映射为分数向量，实现分类。整个流程图如下：</p>
<p><img src="image-4.png" alt="alt text"></p>
<p>注：</p>
<ol>
<li>自己学习 CNN 的时候一开始错误地理解了卷积核的个数和通道数的关系，总结一下：上一层的卷积核的个数作为这一层的输入通道数，这一层的每一个卷积核对所有的输入通道求和，这一层的输出通道数等于这一层的卷积核的个数。</li>
<li>3D CNN 由于其处理的特性，它只能接受结构化的数据（如 Voxel 这样），对于非结构化的数据（如点云，它需要先把点云转为 Voxel.</li>
</ol>
<h1 id="PointNet"><a href="#PointNet" class="headerlink" title="PointNet"></a>PointNet</h1><p>PointNet 直接处理点云数据，点云数据的特点：</p>
<ul>
<li>无序性：每个点之间独立，N 个点输入是无序的，即模型比如对 N! 种不同排列的输入给出同样的输出。</li>
<li>几何变换不变性：对点云在空间上做旋转变换（相当于传感器的位置变化），模型在做分类、分割等任务时应给出同样的结果。</li>
</ul>
<p>PointNet 的思路：点云数据的无序性导致我们应该采用一种对称函数，同时这样的函数要有足够强的拟合能力。于是就有了 PointNet，其整体思路是：对于不同的点先采用同样的多层感知机将其映射到高维空间，然后对其作 max pooling (这显然是一个对称函数), 得到一个向量，可以作为整个点云的语义特征。下面根据不同的任务分别讨论：</p>
<ul>
<li><p>分类任务：直接把这个整体的语义特征过一个 MLP 就得到打分，进行分类即可（输出是概率向量）。公式化如下，式中 $\gamma$, $h$ 均为 MLP：</p>
<p>[<br>f(x_1, x_2,…, x_n) = \gamma(\max_{i = 1, 2,…, n}{h(x_i)})<br>]</p>
</li>
<li>分割任务：分割任务要求对每一个点都要进行分类，所以我们要基于整体语义理解每一个点的信息，因此做一个 concat，即把单点向量与整体语义向量直接拼接后经过 MLP 得到单点特征，再过一个 MLP 得到每一个点的打分（输出是矩阵）。</li>
</ul>
<p>至于空间变换不变性，PointNet 采取了对输入点云预处理的方法以将输入对齐，具体来说：先用一个 T-Net (整个 PointNet 的缩小版，仍含有 MLP, max pooling 等基本结构) 预测一个 3*3 矩阵，再直接将这个矩阵乘以输入点云的位置维度，相当于把输入点云做了一次旋转变换。在点云经过 MLP 提取特征之后，对特征进行同样的操作（这里的矩阵就是特征的维度*特征的维度，这会带来参数量较大，容易过拟合，不易学习的问题，因此作者在这里又针对这个矩阵设置了正则项）。</p>
<p><img src="image-411.png" alt="alt text"></p>
<p>注：映射到高维空间可以增加复杂度增强模型的表达能力，并且原始论文论证了当维度足够高时，PointNet 可以拟合任意的连续集合函数。</p>
<h1 id="PointNet-1"><a href="#PointNet-1" class="headerlink" title="PointNet++"></a>PointNet++</h1><p>PointNet 处理的是 3D 视觉的问题，却抛开了视觉最常用的 CNN 结构，很有创新，但是如果能借鉴一些 CNN 的思想，或许模型的效果还会提高，这就有了 PointNet++。</p>
<p>宏观上看，PointNet++ 是在点云局部递归地调用 PointNet, 使其能发现局部特征并且得到不同层级的特征信息（神似 CNN）。</p>
<h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>在每一次调用 PointNet 时，首先对输入的点云进行采样和分组。采样一般采用最远点采样策略（Farthest Point Sampling, FPS），描述如下：</p>
<ol>
<li>随机选取一个点作为中心点</li>
<li>计算每个点到中心点集中所有点的最近距离</li>
<li>选出最近距离最远的那个点作为下一个中心点，重复 2, 直至选出 N 个中心点。</li>
</ol>
<p>相比于随机采样，或 KNN 采样，这样计算量小且中心点分布更均匀，覆盖性好。</p>
<h2 id="Grouping"><a href="#Grouping" class="headerlink" title="Grouping"></a>Grouping</h2><p>至于分组方法，有两种办法：球查询和 KNN 分组。</p>
<ul>
<li><p>球查询（Query Ball）：设定一个欧氏空间的距离 r, 对于所有与中心点距离小于 r 的点，都归入这个中心点的组，这样适合点云密度均匀的情景，如果密度不均，可能出现一些组点数很少，一些组点数太多。</p>
</li>
<li><p>KNN 分组：与中心点最近的 k 个点归入这个中心点的组，这样保证每个组的半径一样。</p>
</li>
</ul>
<p>在 PointNet++ 论文中，每一组的点云数量为 K，是一个固定值，这是先经过原始分组之后，若小于 K, 则利用 padding 方法补充，若大于 K, 则直接截断。</p>
<p>采样分组之后就对每一个组运用 PointNet, 得到每个组的一个特征，再把这些特征视作点云（当然输入的维数增加了很多），重复采样-分组- PointNet 这个循环即可实现多层特征的提取。（论文中称之为 Sampling Layer - Grouping Layer - PointNet Layer） </p>
<p>从形状上理解：输入为 $N<em>(d + C)$, d 为 3 , C 为原始特征（如法向量、RGB等），经过 Sampling Layer, 得到 $N’</em>(d+C)$, 经过 Grouping Layer, 得到 $N’<em>K</em>(d+C)$, 再经过 PointNet Layer, 得到 $N’*(d+C’)$ （$C’$ 为抽取的高维特征）。</p>
<h2 id="MSG-MRG"><a href="#MSG-MRG" class="headerlink" title="MSG/MRG"></a>MSG/MRG</h2><p>在 Grouping Layer 中还有一个遗留问题，即对于密度非均匀的点云，Query Ball/KNN 都不能很好的处理，在稠密点云上抽取特征的方法可能不能很好地泛化到稀疏点云上。而实际上 LiDAR 传回的点云数据往往是非均匀的，距离近的位置点云稠密，距离远的位置点云稀疏。</p>
<p>对此，作者提出了 Multi-scale grouping (MSG) 和 Multi-resolution grouping (MRG) 两种办法。MSG 就是在每一层都作不同大小的半径进行分组，然后经过 PointNet Layer 抽取的特征作 concat, 结果为总特征。</p>
<p>然而 MSG 相当于每一个循环里的 Grouping 和 PointNet 都要重复很多次，且包含了在较低层级上对较大的半径作 PointNet, 这样的计算开销很大，因此，作者又提出了 MRG. 融合了不同层级的特征. </p>
<p>MRG 思路如下：每一层的特征依然是两个特征的 concat, 第一个特征是这一层的子特征作 PointNet(left), 第二个特征是这一层的前一层的点直接作 PointNet(right). 在稀疏点云中，右侧的特征更可信，稠密点云中，左侧的特征更可信。把这两种不同的特征融合起来，可以使模型更适应不同密度的点云。</p>
<p>下图中 (a) 是 MSG，(b) 是 MRG.</p>
<p><img src="image-417.png" alt="alt text"></p>
<p>PointNet++ 的整体流程图如下：</p>
<p><img src="image-412.png" alt="alt text"></p>
<p>论文中提到，这种分组方法相比于 CNN 的卷积核滑动的办法，能减少 overlap, 保证每个分组之间的独立性，效果更好。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/03/11/RL-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/11/RL-6/" class="post-title-link" itemprop="url">Reinforcement Learning-6: RL for Continuous Control</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-11 17:02:16" itemprop="dateCreated datePublished" datetime="2025-03-11T17:02:16+08:00">2025-03-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-12 22:16:49" itemprop="dateModified" datetime="2025-03-12T22:16:49+08:00">2025-03-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="维数灾难"><a href="#维数灾难" class="headerlink" title="维数灾难"></a>维数灾难</h2><p>在之前的问题中，我们用了 Q-Learning, DQN, Actor-Critic, REINFORCE 等一系列办法，但是策略网络的输出始终是一个向量，每个维度的数值表示对对应动作的打分，这样的动作是离散的，向量的维数等于动作空间的个数。当我们要研究连续控制问题（比如机器臂的转角），我们只能把控制量离散化。但是假设这个机器人的自由度很多，每个控制量都直接离散化研究，这必然使得动作空间极大，很难计算。所以，我们需要研究连续控制算法，<strong>这主要包含 DDPG 和 TD3, 在实践中 TD3 运用较多。</strong></p>
<h2 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h2><p>DDPG（深度确定性策略网络）的整个流程可以如下所示，值得注意的是：<br><img src="\images\image-111.png" alt="alt text"></p>
<ol>
<li>策略网络的输出还是一个向量，不过此向量的维数等于自由度，每一个元素是控制量的一个取值，这就是确定性策略，输出什么执行什么。</li>
<li>DDPG 仍然是一种 AC 类算法，有一个策略网络和一个价值网络。价值网络按照 TD 算法更新，而策略网络依据价值网络更新。</li>
<li>DDPG 是一种异策略，它的目标策略不同于行为策略，可以跑完很多步之后从内存中取出四元组（batch）进行训练，即可以经验回放。</li>
<li>其实从上面几点就可以看出 DDPG 简直是四不像，它融合了之前几乎所有的办法。它有点像 Q学习（或DQN），因为都是异策略，用经验回放更新价值网络；它有点像策略学习，因为采用的是策略网络；它又是一种 AC 类算法。</li>
<li>DDPG 仍然使用了目标网络的办法来降低高估的问题。</li>
<li>DDPG 实践效果并不好，实际中应该用它的改良版 TD3。</li>
</ol>
<p>下面贴一个 DDPG 的伪代码：</p>
<p><img src="\images\image-112.png" alt="alt text"></p>
<h2 id="TD3"><a href="#TD3" class="headerlink" title="TD3"></a>TD3</h2><p>TD3 (Twin Delayed Deep Deterministic Policy Gradient)，译为孪生延迟深度确定性策略梯度算法。TD3 是 DDPG 的改良版，主要做了以下改进：</p>
<ol>
<li>截断的 Double Q-Learning：通过学习两个 Q 值函数，用类似 Double Q-Learning 的方式更新critic 网络。这个目的还是降低高估，训练两个价值网络，让他们做预测，然后选较小的那个作为 TD Target.</li>
<li>延迟策略更新：更新过程中，策略网络的更新频率低于 Q 值网络。这是实验发现，也可以这么解释：当裁判员的打分还不够精确的时候，我们应该先训练裁判员，使之打分准确，然后拿这个裁判去训练选手。所以策略网络应该比价值网络更新的慢一些，而目标网络为了降低高估，更新的也应该慢一些。这样一来，每轮都更新的只有价值网络，而三个目标网络和策略网络是隔几轮更新一次。</li>
<li>目标策略平滑：在目标策略的输出动作中加入噪声，以此平滑 Q 值函数的估计，避免过拟合。这个噪声用的是截断正态分布。</li>
</ol>
<p>TD3 的流程图如下：<br><img src="\images\image-114.png" alt="alt text"></p>
<p>下面贴一个 TD3 的伪代码：<br><img src="\images\image-115.png" alt="alt text"></p>
<p>（以上伪代码摘自《深度强化学习：基础、研究与应用》）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/03/02/RL-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/02/RL-5/" class="post-title-link" itemprop="url">Reinforcement Learning-5: Deep Q Network, Temporal Difference and Q-Learning Algorithm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-03-02 17:02:16 / 修改时间：11:44:42" itemprop="dateCreated datePublished" datetime="2025-03-02T17:02:16+08:00">2025-03-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Policy-Learning-and-Value-Learning"><a href="#Policy-Learning-and-Value-Learning" class="headerlink" title="Policy Learning and Value Learning"></a>Policy Learning and Value Learning</h2><p>深度学习的目标是找到最优的策略，通常有两种办法：策略学习和Q学习。Policy Learning 的优化对象就是策略函数 $\pi(a|s)$, 而 Value Learning 的优化对象是 $Q^*(s,a)$, 目标是找到最准确的 Optimal Action Value 函数，从而据此选择最合适的策略。</p>
<h2 id="Deep-Q-Network-DQN"><a href="#Deep-Q-Network-DQN" class="headerlink" title="Deep Q Network (DQN)"></a>Deep Q Network (DQN)</h2><p>Deep Q-Learning 的目标是，给定状态和动作，预测其可能最高获得回报的期望（最高意味着选择最好的策略）。我们用一个神经网络来预测这个期望。Deep Q-Learning 的方法是，先把 state 输入卷积层，抽取为特征之后，再输入全连接层，最后返回一个分数向量，经过 softmax 函数之后转化为概率值，最后选择概率最大的那一个 action 作为执行动作。（有个小问题：在 LLM 中，为了使生成的文本更灵活丰富，往往不会单纯选择概率最大的输出，而是根据 temperature 和 top_p 参数，从概率最大的几个中采样取出，这样的思路是否能用到 DQN 中呢？这会不会使得 DQN 在训练初期表现得更好？）</p>
<p><img src="\images\image-22.png" alt="alt text"></p>
<p>Deep Q-Learning 网络可以记作 $Q(s,a;\textbf{w})$, 式中 $\textbf{w}$ 表示神经网络的参数（也就是训练的对象）。要训练神经网络，我们应该找出一个损失函数，比如关于预测的回报和真实的回报$y$，我们取：<br>\[<br>L(\textbf{w}) = \frac{1}{2} (Q(s,a ; \textbf{w}) - y)^2<br>\]<br>然后求梯度：<br>\[<br>\nabla_{\textbf{w}}L(\textbf{w}) = (Q(s,a ; \textbf{w}) - y) \cdot \nabla_{\textbf{w}} Q(s,a ; \textbf{w})<br>\]<br>注意，对于一个标量求关于矢量的梯度，梯度的形状与矢量的形状一致。然后做梯度下降（取学习率为 $\alpha$ ）：<br>\[<br>\textbf{w} \leftarrow \textbf{w} - \alpha \cdot \nabla_{\textbf{w}}L(\textbf{w})<br>\]<br>然而这种办法存在一种弊端：每次跑完一个 trajectory（假设episodic）才能做一次更新，优化的效率太低了，有没有其他的办法实现每一步更新一次呢？这就需要 Temporal Difference 算法（时间差分算法）。</p>
<h2 id="Temporal-Difference-Algorithm"><a href="#Temporal-Difference-Algorithm" class="headerlink" title="Temporal Difference Algorithm"></a>Temporal Difference Algorithm</h2><p>时间差分算法实现了每运行一步更新一次神经网络参数。具体方法如下：</p>
<ol>
<li>观察所处的状态 $s_t$</li>
<li>根据 $Q(s_t,a_t;\textbf{w})$, 选择预期回报最大的动作执行</li>
<li>执行一次之后收到了奖励 $r_{t}$, 并且更新到状态 $s_{t+1}$</li>
<li>根据 $Q(s_{t+1},a_{t+1};\textbf{w})$ 预测，并选择出预期回报最大的动作，但是不执行</li>
<li>计算 TD error $\delta_{t} = Q(s_{t},a_{t};\textbf{w}) - [r_{t} + \gamma \cdot Q(s_{t+1},a_{t+1};\textbf{w})]$，其中，$Q(s_{t+1},a_{t+1};\textbf{w})$ 称为 TD Target</li>
<li>计算损失函数：<br>\[<br>L(\textbf{w}) = \frac{1}{2} (Q(s_{t},a_{t};\textbf{w}) - (r_{t} + \gamma \cdot Q(s_{t+1},a_{t+1};\textbf{w})))^2<br>\]</li>
<li>对损失函数求梯度（这里假装 TD Target 关于 $\textbf{w}$ 是一个常数）：<br>\[<br>  \nabla_{\textbf{w}}L(\textbf{w}) = (Q(s_{t},a_{t};\textbf{w}) - (r_{t} + \gamma \cdot Q(s_{t+1},a_{t+1};\textbf{w}))) \cdot \nabla_{\textbf{w}} Q(s,a ; \textbf{w})<br>\]<br>   \[<br>  \nabla_{\textbf{w}}L(\textbf{w}) = \delta_{t} \cdot \nabla_{\textbf{w}} Q(s,a ; \textbf{w})<br>\]</li>
<li>梯度下降：<br>\[<br>\textbf{w} \leftarrow \textbf{w} - \alpha \cdot \delta_{t} \cdot \nabla_{\textbf{w}} Q(s,a ; \textbf{w})<br>\]</li>
</ol>
<p>理解与解释：<br>时间差分算法是比较在每一步操作前和操作后对于最大回报期望的估计，每一步前后的区别是：每一步执行前，对未来的回报估计是纯的估计，而每一步执行后，都能够得到一个立刻的奖励（这个奖励是真实发生的），把这一步的真实奖励和这一步后对未来的估计结合起来，就能得到这一步前对未来的另一个估计。后者的这个估计比前者的估计要准确（毕竟有一部分是真实观测的），所以我们努力使得前者的估计贴近后者的估计，而这就可以用一个误差函数去优化了。</p>
<p>此处还有一个问题：为什么要设置 TD Target: $r_{t} + \gamma \cdot Q(s_{t+1},a_{t+1};\textbf{w})$ 有这样的形式？</p>
<p>回忆一下回报的定义：\[ U_t = \sum_{k=t}^{n} \gamma^{k-t} \cdot R_k \] \[ U_{t+1} = \sum_{k=t+1}^{n} \gamma^{k-t-1} \cdot R_k\]由 $ U_t $ 和 $ U_{t+1}$的定义可得：</p>
<p>\[<br>U_t = R_t + \gamma \cdot \sum_{k=t+1}^{n} \gamma^{k-t-1} \cdot R_k<br>\]</p>
<p>回忆一下，Optimal Action Value Function 可以写成</p>
<p>\[<br>Q_* (s_t, a_t) = \max_{\pi} \mathbb{E} [U_t | S_t = s_t, A_t = a_t]<br>\]</p>
<p>再回忆一下Bellman Optimal Equation, 可以发现：</p>
<p>\[<br>\begin{aligned}<br>Q_* (s_t, a_t) &amp;= \mathbb{E}_{S_{t+1} \sim p(\cdot | s_t, a_t)} [R_t + \gamma \cdot \max_{a \in \mathcal{A}} Q_* (S_{t+1}, a) | S_t = s_t, A_t = a_t].<br>\end{aligned}<br>\]</p>
<p>我们可以用 Monte Carlo 算法对上式进行估计：<br>\[<br>Q_* (s_t, a_t) \approx r_t + \gamma \cdot \max_{a \in \mathcal{A}} Q_* (S_{t+1}, a)<br>\]</p>
<p>再结合到 DQN 中，我们用 $Q(s_{t},a_{t};\textbf{w})$ 来估计 $Q_*(s_{t},a_{t})$, 这就不难理解 TD Target 的形式了。</p>
<h2 id="Training-Process"><a href="#Training-Process" class="headerlink" title="Training Process"></a>Training Process</h2><p>应用 TD 算法进行训练，步骤如下：</p>
<ol>
<li><p><strong>收集训练数据</strong>：我们可以用任何策略函数 $\pi$ 去控制智能体与环境交互，这个 $\pi$ 就叫做行为策略（behavior policy）。比较常用的是 $\epsilon$-greedy 策略：<br>\[<br>a_t =<br>\begin{cases}<br>\arg \max_a Q(s_t, a; w), &amp; \text{以概率 } (1 - \epsilon); \\<br>\text{均匀抽取 } A \text{ 中的一个动作}, &amp; \text{以概率 } \epsilon.<br>\end{cases}<br>\]<br>把智能体在一局游戏中的轨迹记作：<br>\[<br>s_1, a_1, r_1, s_2, a_2, r_2, \cdots, s_n, a_n, r_n.<br>\]<br>把一条轨迹划分成 $n$ 个 $(s_t, a_t, r_t, s_{t+1})$ 这种四元组，存入数组，这个数组叫做经验回放数组（replay buffer）。</p>
</li>
<li><p><strong>更新 DQN 参数</strong> $w$：</p>
<ol>
<li>随机从经验回放数组中取出一个四元组，记作 $(s_j, a_j, r_j, s_{j+1})$。设 DQN 当前的参数为 $w_{\text{now}}$，执行下面的步骤对参数做一次更新，得到新的参数 $w_{\text{new}}$。</li>
<li>对 DQN 做正向传播，得到 Q 值：<br>\[<br>\hat{q}_j = Q(s_j, a_j; w_{\text{now}}) \quad \text{和} \quad \hat{q}_{j+1} = \max_{a \in A} Q(s_{j+1}, a; w_{\text{now}}).<br>\]</li>
<li>计算 TD 目标和 TD 误差：<br>\[<br>\hat{y}_j = r_j + \gamma \cdot \hat{q}_{j+1} \quad \text{和} \quad \delta_j = \hat{q}_j - \hat{y}_j.<br>\]</li>
<li>对 DQN 做反向传播，得到梯度：<br>\[<br>g_j = \nabla_w Q(s_j, a_j; w_{\text{now}}).<br>\]</li>
<li>做梯度下降更新 DQN 的参数：<br>\[<br>w_{\text{new}} \leftarrow w_{\text{now}} - \alpha \cdot \delta_j \cdot g_j.<br>\]</li>
</ol>
</li>
</ol>
<p>智能体收集数据、更新 DQN 参数这两者可以同时进行。可以在智能体每执行一个动作之后，对 $w$ 做几次更新。也可以在每完成一局游戏之后，对 $w$ 做几次更新。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/02/26/RL-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/26/RL-4/" class="post-title-link" itemprop="url">Reinforcement Learning-4: Value Iteration and Policy Iteration</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-02-26 17:02:16 / 修改时间：16:57:20" itemprop="dateCreated datePublished" datetime="2025-02-26T17:02:16+08:00">2025-02-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本节介绍 Value Iteration, Policy Iteration 和 Truncated Policy Iteration. 这三种办法都是解决 Bellman Optimal Equation 的方法，而且都是在 Model-Based 的情况下的算法。</p>
<h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p>下面给出 Value Iteration 的基本流程：</p>
<ol>
<li>给定 State Value 的初始值$v_c(s)$</li>
<li><p>进入循环：</p>
<ol>
<li>Policy Update<br>基于上一轮更新的 State Value, 选定最优策略（贪心策略），即：<br>\[<br>\begin{align}<br>\pi_{k+1} = \mathrm{arg} \max\limits_{\pi}(r_{\pi} + \gamma P_{\pi} v_k) \\<br>\begin{cases}<br>\pi_{k+1}(a|s)  = 1, a = a_k^*(s) \\<br>\pi_{k+1}(a|s)  = 0, a \neq a_k^*(s) \\<br>\end{cases}<br>\end{align}<br>\]<br>式中，$a_k^*(s) = \mathrm{arg} \max\limits_{a}q_k(a,s)$.</li>
<li><p>Value Update<br>在新的 Policy 基础上，更新 State Value，即：<br>\[<br>v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{k}<br>\]</p>
<p>  由于我们采用了贪心策略，因此，上式其实就是：<br>\[<br>v_{k+1} = \max \limits_{a} q(a,s)<br>\]</p>
</li>
<li>Check<br>检查是否满足 $||v_{k+1} - v_{k}|| &lt; \delta $, 其中 $\delta$ 是一个事先指定的很小的正数。如果满足，则退出循环，求解完毕。如果不满足，则继续循环。</li>
</ol>
</li>
</ol>
<h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p>下面给出 Policy Iteration 的基本流程：</p>
<ol>
<li>给定 Policy 的初始值 $\pi_0$</li>
<li>进入循环：<ol>
<li>Policy Evaluation<br>Policy Evaluation 基于上一轮更新的 Policy 解 Bellman Equation.<br>\[<br>v_{\pi_k} = r_{\pi_{k}} + \gamma P_{\pi_{k}} v_{\pi_k}<br>\]<br>一个有趣的问题：此处如何解 Bellman Equation? 方法就是在第2节中所展示的迭代方法，即：<br>\[<br>v_{\pi_k}^{(j+1)} = r_{\pi} + \gamma P_{\pi}v_{\pi_k}^{(j)}<br>\]<br>将这个迭代方法进行很多次，直至 $||v_{\pi_k}^{(j+1)} - v_{\pi_k}^{(j)}|| &lt; \epsilon $, $\epsilon$ 是一个事先指定的很小的正数。</li>
<li>Check<br>检查是否满足 $||v_{\pi_k} - v_{\pi_{k-1}}|| &lt; \delta $, 其中 $\delta$ 是一个事先指定的很小的正数。<ol>
<li>如果此条件满足，则退出循环。</li>
<li>如果此条件不满足，则进行 Policy Improvement.<br>基于求解的 State Value 更新 Policy.<br>\[<br>\begin{align}<br>\pi_{k+1} = \mathrm{arg} \max\limits_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k}) \\<br>\begin{cases}<br>\pi_{k+1}(a|s)  =     1 \ \ \ a = a_{\pi_k}^*(s) \\<br>\pi_{k+1}(a|s)  =     0 \ \ \ a \neq a_{\pi_k}^*(s) \\<br>\end{cases}<br>\end{align}<br>\]</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="Truncated-Policy-Iteration"><a href="#Truncated-Policy-Iteration" class="headerlink" title="Truncated Policy Iteration"></a>Truncated Policy Iteration</h2><p>通过观察可以发现，Policy Iteration 和 Value Iteration 的区别在于，Value Iteration 每次更新 State Value 时，只根据 Bellman Equation 迭代了一次，而 Policy Iteration 从理论上迭代了无穷多次，求出了 Bellman Equation 的一个精确解。因此，我们可以设想有这样一种办法，<strong>它每次根据 Bellman Equation 迭代了有穷多次（不妨记为$j$次），这就是 Truncated Policy Iteration, 如同将 Policy Iteration 截断。</strong> Value Iteration 和 Policy Iteration 可以分别视为 Truncated Policy Iteration 的两个特殊情况。</p>
<p>下面给出 Truncated Policy Iteration 的基本流程：</p>
<ol>
<li>给定 Policy 的初始值 $\pi_0$</li>
<li>进入循环：<ol>
<li>Policy Evaluation<br>Policy Evaluation 基于上一轮更新的 Policy 解 Bellman Equation.<br>\[<br>v_{\pi_k} = r_{\pi_{k}} + \gamma P_{\pi_{k}} v_{\pi_k}<br>\]<br>设定$j = 0$, 进入子循环，检查是否有$j \leq j_{max}$:<ol>
<li>如果$j \leq j_{max}$, 则继续迭代：<br>\[<br>v_{\pi_k}^{(j+1)} = r_{\pi} + \gamma P_{\pi}v_{\pi_k}^{(j)}<br>\]<br>并且更新$j$值：$j = j+1$</li>
<li>如果$j &gt; j_{max}$ 则退出子循环，最终：$v_{\pi_k}= v_{\pi_k}^{j_{max}} $.</li>
</ol>
</li>
<li>Check<br>检查是否满足 $||v_{\pi_k} - v_{\pi_{k-1}}|| &lt; \delta $, 其中 $\delta$ 是一个事先指定的很小的正数。<ol>
<li>如果此条件满足，则退出循环。</li>
<li>如果此条件不满足，则进行 Policy Improvement.<br>基于求解的 State Value 更新 Policy.<br>\[<br>\pi_{k+1} = \mathrm{arg} \max\limits_{\pi}(r_{\pi} + \gamma P_{\pi} v_{\pi_k})<br>\]<br>根据贪心策略：<br>\[<br>\begin{align}<br>\begin{cases}<br>\pi_{k+1}(a|s)  =   1,  a = a_{\pi_k}^*(s)  \\<br>\pi_{k+1}(a|s)  =   0,  a \neq a_{\pi_k}^*(s)  \\<br>\end{cases}<br>\end{align}<br>\]</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>观察 Truncated Policy Iteration 的流程，我们可能有这样的疑问：State Value 的最大迭代次数 $j_{max}$ 到底取多少比较合适呢？</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/02/26/RL-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/26/RL-3/" class="post-title-link" itemprop="url">Reinforcement Learning-3: Bellman Optimal Equation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-02-26 12:02:16 / 修改时间：16:57:46" itemprop="dateCreated datePublished" datetime="2025-02-26T12:02:16+08:00">2025-02-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><p>Bellman Optimal Equation 是在 Bellman Equation 的基础上演变而来。Bellman Equation 阐述了在一只策略和环境的情况下，State Value 所满足的公式。有了State Value之后可以做什么呢？Policy Evaluation! （毕竟强化学习的目标是寻找最优的策略）</p>
<p>下面定义什么是好的策略，如果对于两个策略$\pi_{1}$, $\pi_{2}$ ，下面的式子恒成立：</p>
<p>\[<br>v_{\pi_1}(s) &gt; v_{\pi_2}(s),  \forall s \in \mathcal{S}<br>\]</p>
<p>则称$\pi_{1}$是比$\pi_{2}$更优的策略。强化学习的目标是寻找最优的策略，而最优的策略又依赖于选择最合适的状态作为下一次迁移的目标（依赖于State Value），这就需要把 State Value 和 Policy 同时优化，这就是 Bellman Optimal Equation.</p>
<h2 id="Bellman-Optimal-Equation"><a href="#Bellman-Optimal-Equation" class="headerlink" title="Bellman Optimal Equation"></a>Bellman Optimal Equation</h2><p>Bellman Equation 如下：</p>
<p>\[<br>\begin{align}<br>v_{\pi}(s)<br>&amp; = \sum \limits_{a}^{} \pi(a | s) \left[\sum \limits_{r} r \cdot p(r | s, a) + \gamma  \sum \limits_{s’} v_{\pi}(s’)  p(s’|s, a)\right]  \\<br>\end{align}<br>\]</p>
<p>Bellman Optimal Equation 如下：</p>
<p>\[<br>\begin{align}<br>v_{\pi}(s)<br>&amp; = \max \limits_{\pi} \sum \limits_{a}^{} \pi(a | s) \left[\sum \limits_{r} r \cdot p(r | s, a) + \gamma  \sum \limits_{s’} v_{\pi}(s’)  p(s’|s, a)\right]  \\<br>&amp; = \max \limits_{\pi} \sum \limits_{a}^{} \pi(a | s) \cdot q(s, a)  \\<br>\end{align}<br>\]</p>
<p>其实就是进行了一步优化，它表达的是在最优策略下的 State Value 之间的关系。这里面有两个未知量，一个是策略$\pi$，一个是 State Value, 现在要对这两个未知量<strong>同时优化</strong>。</p>
<p>首先要解决的是如果已知 Action Value, 如何找到最合适的策略的问题，这就是一个简单的线性规划问题。这里我们以 3 种 Action 的情况为例。</p>
<p>\[<br>\begin{align}<br>\max\limits_{\pi(a_i | s)} \pi(a_1 | s)q(s, a_1) + \pi(a_2 | s)q(s, a_2) + \pi(a_3 | s)q(s, a_3) \\<br>\mathrm{s.t.} \ \pi(a_1 | s) + \pi(a_2 | s) + \pi(a_3 | s) = 1<br>\end{align}<br>\]</p>
<p>如果 Action Value 给定，且已知：</p>
<p>\[<br>q(s, a_1) \geq q(s, a_2) \geq q(s, a_3)<br>\]</p>
<p>那么，显然最优解在 $\pi(a_1 | s) = 1$, $\pi(a_j | s) = 0 (j = 2,3) $ 处取到。因此我们把这种办法（其实是一种deterministic policy）称为 <strong>Greedy Policy</strong>. 即：</p>
<p>\[<br>\begin{align}<br>\begin{cases}<br>\pi(a|s) = 1  \ \  (a = a^*)  \\<br>\pi(a|s) = 0  \ \  (a \neq a^*)  \\<br>\end{cases}<br>\end{align}<br>\]</p>
<p>式中，$a^*= \mathrm{arg} \max \limits_{a} q(s,a) $，那么 Bellman Optimal Equation 可以重新写作：</p>
<p>\[<br>\begin{align}<br>v_{\pi}(s)<br>&amp; =  \max\limits_{a \in \mathcal{A}} q(s, a)  \\<br>&amp; =  q(s, a^*)  \\<br>\end{align}<br>\]</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/02/23/RL-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/23/RL-2/" class="post-title-link" itemprop="url">Reinforcement Learning-2: Bellman Equation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-23 12:02:16" itemprop="dateCreated datePublished" datetime="2025-02-23T12:02:16+08:00">2025-02-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-26 02:47:40" itemprop="dateModified" datetime="2025-02-26T02:47:40+08:00">2025-02-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Motivating-Example"><a href="#Motivating-Example" class="headerlink" title="Motivating Example"></a>Motivating Example</h2><p><strong>Calculating return is important to evaluate a policy.</strong> 因此，为了比较以下三个策略，利用Discounted Return，很容易分析得，第一个策略是最好的。<br><img src="/images/image-16.png" alt="alt text"></p>
<p>对于第三个策略的Return，其实是一种期望。这可以作为理解Bellman Equation的一个引子。</p>
<h2 id="Bootstrapping"><a href="#Bootstrapping" class="headerlink" title="Bootstrapping"></a>Bootstrapping</h2><p>Bootstrapping是一种计算Discounted Return的办法。设置一个较为简单的循环policy作为演示：<br><img src="/images/image-15.png" alt="alt text"></p>
<p>根据Discounted Return的定义，可以列出：<br>\[<br>\begin{align}<br>v_1 = r_1 + \gamma r_2 + \gamma^2 r_3 + \dots \\<br>v_2 = r_2 + \gamma r_3 + \gamma^2 r_4 + \dots \\<br>v_3 = r_3 + \gamma r_4 + \gamma^2 r_1 + \dots \\<br>v_4 = r_4 + \gamma r_1 + \gamma^2 r_2 + \dots \\<br>\end{align}<br>\]<br>仔细观察之后，可以发现有一种整体替代的办法，把无穷项化为有限项：<br>\[<br>\begin{align}<br>v_1 = r_1 + \gamma v_2 \\<br>v_2 = r_2 + \gamma v_3 \\<br>v_3 = r_3 + \gamma v_4 \\<br>v_4 = r_4 + \gamma v_1 \\<br>\end{align}<br>\]<br>这种办法可以用矩阵表达，如下：<br>\[<br>\begin{pmatrix}<br>v_1 \\<br>v_2 \\<br>v_3 \\<br>v_4 \\<br>\end{pmatrix}<br>=\begin{pmatrix}<br>r_1 \\<br>r_2 \\<br>r_3 \\<br>r_4 \\<br>\end{pmatrix}+\gamma<br>\begin{pmatrix}<br>0 &amp; 1 &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; 1 &amp; 0 \\<br>0 &amp; 0 &amp; 0 &amp; 1 \\<br>1 &amp; 0 &amp; 0 &amp; 0 \\<br>\end{pmatrix}<br>\begin{pmatrix}<br>v_1 \\<br>v_2 \\<br>v_3 \\<br>v_4 \\<br>\end{pmatrix}<br>\]<br>也可以写作：<br>\[<br>\textbf{v} = \textbf{r} + \gamma \textbf{Pv}<br>\]<br>这其实就是对于deterministic cases的Bellman Equation，它的解为：<br>\[<br>\textbf{v} = (\textbf{I} - \gamma \textbf{P})^{-1}\textbf{r}<br>\]<br><strong>这种整体替代的办法就是Bootstrapping，其核心就是每一个state的Discounted Return依赖于其他state的Discounted Return。</strong></p>
<h2 id="State-Value"><a href="#State-Value" class="headerlink" title="State Value"></a>State Value</h2><p>这一小节当中，我们要从deterministic cases过渡到stochastic cases中去，对于这两种情况，强化学习的各种概念的含义是保持一致的，只是每一个定量都变为了随机变量。而相应的，Discounted Return的概念要过渡到State Value，这是强化学习最核心的概念之一。</p>
<h3 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h3><p>把RL中的定量重新定义成随机变量，考虑MDP中的一步：<br>\[<br>S_t \xrightarrow{A_t} R_{t+1}, S_{t+1}<br>\]</p>
<ul>
<li>$t$, $t+1$表示离散化的时间。</li>
<li>$S_t$ 表示时间$t$时，agent的状态。</li>
<li>$A_t$ 表示时间$t$时，agent的所采取的行动。</li>
<li>$R_{t+1}$ 表示时间$t$时，agent的状态从$S_t$迁移至$S_{t+1}$所获得的奖励，有时也写作$R_{t}$.<br>注意，$S_t$, $A_t$, $R_{t+1}$均是随机变量。这一步演化过程受概率分布的控制。</li>
<li>$ S_t \xrightarrow{} A_{t} $ is governed by $\pi(A_t = a | S_t = s)$</li>
<li>$ S_t, A_{t} \xrightarrow{} R_{t+1} $ is governed by $p(R_{t+1} = r | S_t = s, A{t} = a)$</li>
<li>$ S_t, A_{t} \xrightarrow{} S_{t+1} $ is governed by $p(S_{t+1} = s’ | S_t = s, A{t} = a)$</li>
</ul>
<p>这里有一个很有趣的问题，既然演化过程中所采取的行动、下一步的状态、所获得奖励都受概率分布的控制，那么我们为什么不统一地用$p(\dots | \dots)$表达，而对于策略专门用了$\pi$表示呢？笔者思考了一下，认为决定的所采取的行动的策略($\pi(A_t = a | S_t = s)$)从本质上不同于$p(R_{t+1} = r | S_t = s, A{t} = a)$, $p(S_{t+1} = s’ | S_t = s, A{t} = a)$. 后两者描述的是agent采取行动之后环境与agent的交互情况，这实际上是取决于环境本身（比如grid-world当中网格的构造，哪里是target area，哪里是forbidden area）以及人们基于训练目标所设定的激励政策。这些内容可能在一定程度上是固定的（比如一个trajectory中），虽然其分布有可能知道，也有可能不知道（后面会解释）。而策略$\pi$则完全由agent学习而得，在学习过程中应当是不断变化的，才能取得进步。</p>
<h3 id="Definition-of-State-Value"><a href="#Definition-of-State-Value" class="headerlink" title="Definition of State Value"></a>Definition of State Value</h3><p>考虑一个trajectory:<br>\[<br>S_t \xrightarrow{A_t} R_{t+1}, S_{t+1} \xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \xrightarrow{A_{t+2}} R_{t+3}, S_{t+3} \dots<br>\]<br>Discounted Return 是：<br>\[<br>G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots<br>\]<br>显然，$R_{i}$是一个随机变量，那么$G_t$也是一个随机变量。而它的期望就被定义为<strong>在状态$s$下，基于策略$\pi$的State Value</strong>: $v_{\pi}(s)$.<br>\[<br>v_{\pi}(s) = \mathbb{E}[G_t | S_t = s]<br>\]</p>
<ul>
<li>State Value 是关于状态$s$的函数，评价了一个状态的价值。</li>
<li>State Value 也是策略$\pi$的函数，它的评价是基于目前选择的这个策略的。状态给定的情况下，基于某一个策略得到的State Value更高，证明这个策略更好，follow这条策略更有可能得到更高的return.</li>
<li>Discounted Return 和 State Value 的关系：对于 deterministic cases, Discounted Return 和 State Value 没有区别，而对于 stochastic cases, State Value 就是 Discounted Return 的期望。</li>
</ul>
<h2 id="Establishment-of-Bellman-Equation"><a href="#Establishment-of-Bellman-Equation" class="headerlink" title="Establishment of Bellman Equation"></a>Establishment of Bellman Equation</h2><p>由上一小节，Discounted Return 是：<br>\[<br>G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots<br>\]<br>使用Bootstrapping方法，可得：<br>\[<br>G_t = R_{t+1} + \gamma G_{t+1}<br>\]<br>再结合State Value的定义，可得：<br>\[<br>\begin{align}<br>v_{\pi}(s) &amp; = \mathbb{E}[G_t | S_t = s] \\<br>&amp; = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\<br>&amp; = \mathbb{E}[R_{t+1}  | S_t = s]  + \gamma \mathbb{E}[G_{t+1}| S_t = s]\\<br>\end{align}<br>\]<br>下面分别计算：<br>\[<br>\begin{align}<br>\mathbb{E}[R_{t+1}  | S_t = s]  = \sum \limits_{a}^{} \pi(a | s) \sum \limits_{r} r \cdot p(r | s, a)<br>\end{align}<br>\]<br>\[<br>\begin{align}<br>\mathbb{E}[G_{t+1}| S_t = s] &amp; = \sum \limits_{s’} \mathbb{E}[G_{t+1}| S_t = s, S_{t+1} = s’] p(s’|s) \\<br>&amp; = \sum \limits_{s’} \mathbb{E}[G_{t+1}|  S_{t+1} = s’] p(s’|s) \\<br>&amp; = \sum \limits_{s’} v_{\pi}(s’) p(s’|s) \\<br>&amp; = \sum \limits_{s’} v_{\pi}(s’) \sum \limits_{a} p(s’|s, a) \pi(a|s) \\<br>\end{align}<br>\]<br>因此：<br>\[<br>\begin{align}<br>v_{\pi}(s) &amp; = \mathbb{E}[R_{t+1}  | S_t = s]  + \gamma \mathbb{E}[G_{t+1}| S_t = s]\\<br>&amp; = \sum \limits_{a}^{} \pi(a | s) \sum \limits_{r} r \cdot p(r | s, a) + \gamma \sum \limits_{a} \pi(a|s) \sum \limits_{s’} v_{\pi}(s’)  p(s’|s, a)  \\<br>&amp; = \sum \limits_{a}^{} \pi(a | s) \left[\sum \limits_{r} r \cdot p(r | s, a) + \gamma  \sum \limits_{s’} v_{\pi}(s’)  p(s’|s, a)\right]  \\<br>\end{align}<br>\]<br>这就得到了<strong>Bellman Equation</strong>。</p>
<ul>
<li><p>对于每一个状态均可以写出来一个Bellman Equation，因此，这一组方程不同状态的State Value之间的关系，即Bootstrapping方法。</p>
</li>
<li><p>Bellman Equation是基于策略$\pi(a|s)$的，因此，求解Bellman Equation又被称作policy evaluation.</p>
</li>
<li><p>$p(s’|s, a), p(r | s, a)$这两项指的是我们对于agent处于某一状态下，采取了某一行动之后，所获得奖励和所迁移到的下一状态的概率分布。我们对这个概率分布是否完全掌握，从本质上反映了我们对于与agent交互的环境是否有完全的了解(即Environment Model是否建立)。如果对所处的环境没有完全掌握，agent只能通过不断试验，观测所处的环境。<strong>从这一判断标准出发，强化学习可分为Model-Based RL和Model-Free RL.</strong></p>
</li>
</ul>
<h2 id="Matrix-Vector-Form-of-the-Bellman-Equation"><a href="#Matrix-Vector-Form-of-the-Bellman-Equation" class="headerlink" title="Matrix-Vector Form of the Bellman Equation"></a>Matrix-Vector Form of the Bellman Equation</h2><h3 id="Rewrite-the-Bellman-Equation-in-Matrix-Vector-Form"><a href="#Rewrite-the-Bellman-Equation-in-Matrix-Vector-Form" class="headerlink" title="Rewrite the Bellman Equation in Matrix-Vector Form"></a>Rewrite the Bellman Equation in Matrix-Vector Form</h3><p>对Bellman Equation作如下代换：<br>\[<br>\begin{align}<br>r_{\pi}(s) = \sum \limits_{a} \pi(a|s) \sum \limits_{r} p(r|s,a) \cdot r \\<br>p_{\pi}(s’ | s) = \sum \limits_{a} \pi(a|s) p(s’|s,a)<br>\end{align}<br>\]</p>
<p>式中，$p_{\pi}(s’ | s)$是follow策略$\pi$下，从状态$s$迁移到$s’$的概率；$r_{\pi}(s)$是follow策略$\pi$下，状态$s$这一步得到奖励的期望。因此，可以将Bellman Equation写成：<br>\[<br>v_{\pi}(s_i) = r_{\pi}(s_i) + \gamma \sum \limits_{s_j} p_{\pi}(s_j | s_i) v_{\pi}(s_j)<br>\]</p>
<p>写成矩阵形式为：<br>\[<br>\textbf{v}_{\pi} = \textbf{r}_{\pi} + \gamma \textbf{P}_{\pi}\textbf{v}_{\pi}<br>\]</p>
<p>式中，$[\textbf{P}_{\pi}]_{ij} = p_{\pi}(s_j | s_i)$，被称为<strong>State Transition Matrix</strong>.</p>
<h3 id="Solution-of-the-Bellman-Equation"><a href="#Solution-of-the-Bellman-Equation" class="headerlink" title="Solution of the Bellman Equation"></a>Solution of the Bellman Equation</h3><p>从数学上，Bellman Equation的解可写作：<br>\[<br>\textbf{v}_{\pi} = (\textbf{I} - \gamma \textbf{P}_{\pi}) ^{-1} \textbf{r}_{\pi}<br>\]</p>
<p>而实际上，由于$\textbf{v}_{\pi}$常常维数很高，上面的逆矩阵很难用计算机求解，我们常采用迭代的方法求解：<br>\[<br>\textbf{v}_{k+1} = \textbf{r}_{\pi} + \gamma \textbf{P}_{\pi}\textbf{v}_{k}<br>\]</p>
<p>从数学上，可以证明这种迭代的方法下，最终会收敛到Bellman Equation的解（证明略）。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/02/22/RL-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/22/RL-1/" class="post-title-link" itemprop="url">Reinforcement Learning-1: Basic Concepts</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-22 16:48:16" itemprop="dateCreated datePublished" datetime="2025-02-22T16:48:16+08:00">2025-02-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-25 12:22:28" itemprop="dateModified" datetime="2025-02-25T12:22:28+08:00">2025-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h2><h3 id="A-grid-world-example"><a href="#A-grid-world-example" class="headerlink" title="A grid-world example"></a>A grid-world example</h3><ul>
<li><strong>State: The status of the agent with respect to the environment.</strong><ul>
<li>在grid-world示例中，agent的位置就是state.</li>
</ul>
</li>
<li><strong>State Space</strong>: 所有state构成的一个集合就是State Space.</li>
<li><strong>Action: For each state, there are some possible actions.</strong><ul>
<li>在grid-world示例中，agent每一步可采取的行动（如向上走、向下走）就是action.</li>
</ul>
</li>
<li><strong>Action Space</strong>:所有action构成的一个集合就是Action Space.<br>Action Space与当前的state有关。</li>
<li><strong>State Transition</strong>: When taking an action, the agent may move from one state to another. Such a process is called <em>state transition</em>.<br>\[<br>s_1 \xrightarrow{a_2} s_2<br>\]</li>
<li>Tabular reprentation: 用表格的形式表达状态转移，但是只能表达deterministic cases。</li>
<li><p>State transition probability: 用条件概率才能表达stochastic cases.<br>\[<br>p(s_2 | s_1, a_2) = 0.4 \\<br>p(s_1 | s_1, a_2) = 0.6<br>\]</p>
</li>
<li><p><strong>Policy: Policy tells the agent what actions to take at a state.</strong></p>
<ul>
<li>基于policy可以生成trajectory.</li>
<li>仍然用条件概率表达stochastic policy.<br>\[<br>\pi(a_1 | s_1) = 0.5<br>\]</li>
<li>Tabular representation of a policy 既可以描述deterministic policy，也可以描述stochastic policy.</li>
<li>编程时用随机数描述概率的问题。</li>
</ul>
</li>
<li><strong>Reward: A real number we get after taking an action.</strong><ul>
<li>正的reward表示鼓励，负的reward表示惩罚。</li>
<li>Reward可以理解为人机交互的一个接口。</li>
<li>Tabular representation of a reward 只能描述deterministic reward.<br>\[<br>p(r = -1 | s_1, a_1) = 1<br>\]</li>
<li>努力学习应该会获得奖励，但是奖励的多少是不一定的。</li>
<li>鼓励行动，而非鼓励下一个状态。（注重过程而非结果）</li>
</ul>
</li>
<li><strong>Trajectory: A trajectory is a state-action-reward chain.</strong><br>\[<br>s_1 \xrightarrow{a_2, r=0} s_2 \xrightarrow{a_3, r=0} s_5 \xrightarrow{a_3, r=0} s_8 \xrightarrow{a_2, r=1} s_9<br>\]</li>
<li>Return: 在一个trajectory中所有reward的和即为return.<ul>
<li>用于评估一个策略的好坏。</li>
</ul>
</li>
<li>Discounted Return:<br>\[<br>\mathrm{Discounted\ Return} = \Sigma \gamma^{i}r_i<br>\]<ul>
<li>$\gamma$ 被称为Discounted Rate, $\gamma$ 减少，可能会更加近视；$\gamma$ 增加，可能会较为远视。</li>
</ul>
</li>
<li><strong>Episode: When interacting with the environment following a policy, the agent may stop at some <em>terminal states.</em> The resulting trajectory is called an episode(or a trial).</strong><ul>
<li>Episode is a finite trajectory.(有限的任务被称为episodic tasks，永远进行下去的任务是continuing tasks)</li>
<li>有两种将episodic tasks转化为continuing tasks的办法：<ul>
<li>第一种设置terminal state的action space只有留在原地，且奖励始终为0.</li>
<li>第二种设置terminal state仍然是一个正常的state, 仍然有可能再离开terminal state, 再次进入时会获得正的奖励。</li>
<li>第二种方法更加鼓励积极的探索，学习效果更好。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Markov-Decision-Process-MDP"><a href="#Markov-Decision-Process-MDP" class="headerlink" title="Markov Decision Process (MDP)"></a>Markov Decision Process (MDP)</h3><h4 id="Key-Elements"><a href="#Key-Elements" class="headerlink" title="Key Elements"></a>Key Elements</h4><ul>
<li>Sets:<ul>
<li>Set of States: $\mathcal{S}$</li>
<li>Set of Actions: $\mathcal{A}(s)$ is associated for state $s\in \mathcal{S}$.</li>
<li>Set of Rewards: $\mathcal{R}(s,a)$</li>
</ul>
</li>
<li>Probability Distribution:<ul>
<li>State transition probability: $p(s’|s,a)$</li>
<li>Reward Probability: $p(r |s,a)$</li>
</ul>
</li>
<li>Policy: $\pi(a|s)$</li>
<li><strong><em>Markov Property</em>: memoryless property</strong><br>\[<br>\begin{align}<br>p(s_{t+1}|a_{t+1}, s_{t},\dots,a_1,s_0) = p(s_{t+1}|a_{t+1}, s_{t}) \\<br>p(r_{t+1}|a_{t+1}, s_{t},\dots,a_1,s_0) = p(r_{t+1}|a_{t+1}, s_{t})<br>\end{align}<br>\]</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/02/09/CLIPBasedAudioProcessingSystem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/09/CLIPBasedAudioProcessingSystem/" class="post-title-link" itemprop="url">一种基于CLIP模型的智能视频帧匹配系统</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-02-09 14:22:35 / 修改时间：14:23:15" itemprop="dateCreated datePublished" datetime="2025-02-09T14:22:35+08:00">2025-02-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h2><p>在视频内容分析领域，我们常常需要从长视频中快速定位特定场景或画面。传统方法依赖于人工逐帧查看或基于元数据的简单搜索，效率低下且无法应对复杂语义查询。CLIP（Contrastive Language-Image Pretraining）模型的出现为这一领域带来了新的可能性，它能够理解图像与文本之间的语义关联。</p>
<p>笔者基于HuggingFace的CLIP模型开发了智能视频帧匹配系统，该系统可实现：</p>
<ol>
<li><strong>自动解析视频文件</strong>（支持MP4/AVI/MOV等常见格式）</li>
<li><strong>实时计算文本描述与视频帧的语义相似度</strong></li>
<li><strong>智能定位最佳匹配帧并支持可视化预览</strong></li>
<li><strong>一键保存关键帧为JPG图片</strong></li>
</ol>
<h2 id="开发思路"><a href="#开发思路" class="headerlink" title="开发思路"></a>开发思路</h2><h3 id="本地模型加载模块"><a href="#本地模型加载模块" class="headerlink" title="本地模型加载模块"></a>本地模型加载模块</h3><p>考虑到实际部署时的网络稳定性问题，系统采用本地化模型加载方案：</p>
<ul>
<li><strong>使用<code>transformers</code>库加载预训练的CLIP模型</strong>（clip-vit-base-patch32）</li>
<li><strong>设置本地模型缓存路径</strong>（E:\clip），包含完整的模型文件：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LOCAL_MODEL_DIR = <span class="string">r&quot;E:\clip&quot;</span></span><br><span class="line">model = CLIPModel.from_pretrained(<span class="string">&quot;openai/clip-vit-base-patch32&quot;</span>).to(device)</span><br></pre></td></tr></table></figure></li>
<li><strong>异常处理机制</strong>：模型加载失败时，提示缺失文件清单</li>
</ul>
<h3 id="多模态编码模块"><a href="#多模态编码模块" class="headerlink" title="多模态编码模块"></a>多模态编码模块</h3><h4 id="文本编码"><a href="#文本编码" class="headerlink" title="文本编码"></a><strong>文本编码</strong></h4><p>文本编码是将用户输入的英文描述转化为高维语义向量的过程。CLIP模型使用Transformer架构对文本进行编码，具体步骤如下：</p>
<ul>
<li><strong>文本预处理</strong>：将用户输入的英文描述（如 “a waitress standing in front of a restaurant”）进行分词和标准化处理。</li>
<li><strong>Tokenization</strong>：将文本转换为模型可理解的Token序列，每个Token对应一个唯一的ID。</li>
<li><strong>特征提取</strong>：通过多层Transformer编码器，将Token序列映射为固定长度的语义向量（本项目中维数为512）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = processor(text=texts, return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="literal">True</span>).to(device)</span><br><span class="line">text_features = model.get_text_features(**inputs)</span><br></pre></td></tr></table></figure>
<h4 id="图像编码"><a href="#图像编码" class="headerlink" title="图像编码"></a><strong>图像编码</strong></h4><p>图像编码是将视频帧转化为视觉特征向量的过程。CLIP模型使用Vision Transformer（ViT）架构对图像进行编码，具体步骤如下：</p>
<ul>
<li><strong>图像预处理</strong>：将视频帧从BGR格式转换为RGB格式，并调整大小为模型输入要求。</li>
<li><strong>分块处理</strong>：将图像划分为多个小块（Patch），每个小块作为一个输入单元。</li>
<li><strong>特征提取</strong>：通过多层Transformer编码器，将图像块序列映射为固定长度的视觉特征向量（本项目中维数为512）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = processor(images=[pil_image], return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="literal">True</span>).to(device)</span><br><span class="line">image_features = model.get_image_features(**inputs)</span><br></pre></td></tr></table></figure>
<h4 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h4><p>相似度是衡量文本描述与视频帧之间语义匹配程度的关键步骤。CLIP模型通过以下方式实现：</p>
<ul>
<li><strong>向量对齐</strong>：将文本特征向量和图像特征向量映射到同一语义空间。</li>
<li><strong>余弦相似度</strong>：计算两个向量的余弦相似度，公式如下：<br><img src="/images/image-9.png" alt="alt text"></li>
<li><strong>匹配分数</strong>：相似度值范围在[-1, 1]之间，值越接近1表示匹配度越高。</li>
</ul>
<h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPModel, CLIPProcessor</span><br><span class="line"></span><br><span class="line">LOCAL_MODEL_DIR = <span class="built_in">input</span>(<span class="string">&quot;请输入你的CLIP模型路径：&quot;</span>)</span><br><span class="line"></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using device: <span class="subst">&#123;device&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    model = CLIPModel.from_pretrained(<span class="string">&quot;openai/clip-vit-base-patch32&quot;</span>).to(device)</span><br><span class="line">    processor = CLIPProcessor.from_pretrained(<span class="string">&quot;openai/clip-vit-base-patch32&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;加载模型失败，请检查本地文件是否完整: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">&quot;必须包含以下文件: config.json, pytorch_model.bin, preprocessor_config.json, tokenizer_config.json, vocab.json, merges.txt&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">video_path = <span class="built_in">input</span>(<span class="string">&quot;请输入你的视频文件路径：&quot;</span>)</span><br><span class="line">output_frame_path =<span class="built_in">input</span>(<span class="string">&quot;请输入你的最佳匹配帧保存路径：&quot;</span>)  </span><br><span class="line"></span><br><span class="line">user_text = <span class="built_in">input</span>(<span class="string">&quot;请输入一段英文描述（例如：a waitress standing in front of a restaurant）：&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> user_text.strip():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;输入不能为空！&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">texts = [user_text]</span><br><span class="line"></span><br><span class="line">cap = cv2.VideoCapture(video_path)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> cap.isOpened():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;无法打开视频文件: <span class="subst">&#123;video_path&#125;</span>&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">best_match_score = -<span class="number">1</span></span><br><span class="line">best_match_frame = <span class="literal">None</span></span><br><span class="line">best_match_text = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">frame_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    frame_count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Processing frame <span class="subst">&#123;frame_count&#125;</span>...&quot;</span>)</span><br><span class="line"></span><br><span class="line">    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)</span><br><span class="line">    pil_image = Image.fromarray(frame_rgb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        inputs = processor(</span><br><span class="line">            text=texts,</span><br><span class="line">            images=[pil_image],</span><br><span class="line">            return_tensors=<span class="string">&quot;pt&quot;</span>,</span><br><span class="line">            padding=<span class="literal">True</span></span><br><span class="line">        ).to(device)</span><br><span class="line">    <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;预处理失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            outputs = model(**inputs)</span><br><span class="line">        <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;推理错误: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    logits_per_image = outputs.logits_per_image.cuda().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(texts) == <span class="number">1</span>:</span><br><span class="line">        current_match_score = logits_per_image[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        probs = np.exp(logits_per_image) / np.<span class="built_in">sum</span>(np.exp(logits_per_image), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        current_match_score = probs[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    current_match_text = texts[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> current_match_score &gt; best_match_score:</span><br><span class="line">        best_match_score = current_match_score</span><br><span class="line">        best_match_frame = frame</span><br><span class="line">        best_match_text = current_match_text</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;更新最佳匹配: &#x27;<span class="subst">&#123;best_match_text&#125;</span>&#x27; (分数: <span class="subst">&#123;best_match_score:<span class="number">.4</span>f&#125;</span>)&quot;</span>)</span><br><span class="line"></span><br><span class="line">cap.release()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> best_match_frame <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n最佳匹配描述: &#x27;<span class="subst">&#123;best_match_text&#125;</span>&#x27; (分数: <span class="subst">&#123;best_match_score:<span class="number">.4</span>f&#125;</span>)&quot;</span>)</span><br><span class="line">    cv2.imshow(<span class="string">&quot;Best Matching Frame&quot;</span>, best_match_frame)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line">    save_frame = <span class="built_in">input</span>(<span class="string">&quot;是否保存最佳匹配帧？(y/n): &quot;</span>).strip().lower()</span><br><span class="line">    <span class="keyword">if</span> save_frame == <span class="string">&#x27;y&#x27;</span>:</span><br><span class="line">        cv2.imwrite(output_frame_path, best_match_frame)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;最佳匹配帧已保存到: <span class="subst">&#123;output_frame_path&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;未找到匹配的帧。&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h2><p><strong>环境要求：</strong></p>
<ul>
<li>请安装Python 3.8+</li>
<li>安装依赖库：<code>torch</code>, <code>transformers</code>,<code>opencv-python</code>,<code>pillow</code></li>
<li>如希望利用显卡进行加速计算，请安装CUDA和CUDNN</li>
</ul>
<p><strong>准备步骤：</strong></p>
<ul>
<li>下载CLIP模型文件至本地目录</li>
<li>准备一个您要处理的视频，并确保视频文件路径不包含中文</li>
<li>准备一个保存最佳匹配帧的路径</li>
</ul>
<h2 id="输出结果示例"><a href="#输出结果示例" class="headerlink" title="输出结果示例"></a>输出结果示例</h2><p>笔者以本人所在学院的宣传片为例，其中有一段是游泳的片段，笔者尝试利用该程序找到游泳的片段。</p>
<p>首先按照要求输入CLIP模型的地址、视频地址和导出图片的地址，然后系统开始运行。<br><img src="/images/image-4.png" alt="alt text"></p>
<p>等到处理至游泳片段后，匹配分数不断刷新。<br><img src="/images/image-5.png" alt="alt text"></p>
<p>程序运行完毕，最匹配的那一帧会自动弹出。<br><img src="/images/image-6.png" alt="alt text"><br><img src="/images/image-7.png" alt="alt text"></p>
<p>键盘输入<code>y</code>之后，图片自动保存在设置的路径中。<br><img src="/images/image-8.png" alt="alt text"></p>
<h2 id="技术优势"><a href="#技术优势" class="headerlink" title="技术优势"></a>技术优势</h2><ul>
<li><strong>多模态对齐</strong>：CLIP模型通过对比学习实现了文本和图像在语义空间的对齐。</li>
<li><strong>高效计算</strong>：利用GPU加速，实时处理视频帧并计算匹配分数。</li>
<li><strong>语义理解</strong>：能够捕捉复杂的语义关系，而不仅仅是简单的关键词匹配。</li>
</ul>
<h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><p>本项目是笔者在学习了一点大模型技术后，利用不到半天时间开发的小型应用。这只是使用多模态大模型的一个小练习，且市场上已经有一些不错的产品实现了本项目的功能（如百度网盘自带的“云一朵”助理）。</p>
<p>这篇文档会先在笔者的个人博客上发布，稍后会整理并发布在<a target="_blank" rel="noopener" href="https://github.com/Dasai-Hzm">我的Github</a>上，欢迎读者在Github社区提交issue讨论改进方案。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/02/08/EssayReadingHelper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/08/EssayReadingHelper/" class="post-title-link" itemprop="url">一种基于大模型的自动化批量文献概要生成系统</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-08 20:03:16" itemprop="dateCreated datePublished" datetime="2025-02-08T20:03:16+08:00">2025-02-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-09 13:01:22" itemprop="dateModified" datetime="2025-02-09T13:01:22+08:00">2025-02-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h2><p>在实际的科研场景中，我们常常需要阅读大量的文献，这其中有一个文献的筛选过程：我们希望快速地知道这篇文章讲了哪些内容，从而判断这篇文章与自己的方向是否契合，决定是否要精读。虽然，阅读文章的摘要就能做到这一点，但是摘要有时不能覆盖全文每一个部分的主题，提供的信息可能不足，而且纯英文的摘要对于刚刚入门该领域的科研人员并不友好。</p>
<p>笔者基于阿里通义大模型 (qwen-turbo-2024-11-01) 开发了这样一种自动化文献处理系统，它可以实现大批量纯英文文献的阅读，并以pdf格式输出文章的概要，概要有条理且内容清晰，总长度不超过2页A4纸。</p>
<p>科研人员可以大批量下载某一特定方向的论文，然后先经过本系统处理为中文概要进行阅读，再选择其中合适的论文用一些大模型论文阅读插件阅读（或自行阅读），从而在一定程度上提高了科研工作者的论文阅读效率。</p>
<h2 id="开发思路"><a href="#开发思路" class="headerlink" title="开发思路"></a>开发思路</h2><h3 id="PDF文本提取模块-extract-useful-text-from-pdf"><a href="#PDF文本提取模块-extract-useful-text-from-pdf" class="headerlink" title="PDF文本提取模块 (extract_useful_text_from_pdf)"></a>PDF文本提取模块 (extract_useful_text_from_pdf)</h3><p>第一步是解析PDF，由于大模型对PDF的解析较慢且准确率不高，笔者决定先自行解析PDF为文本，再向大模型中输入文本。笔者采用<code>PyPDF2</code>库实现PDF解析，通过逐页处理、首尾行剔除策略去除页眉页脚。该模块处理单页时保留段落结构，输出文本已去除冗余格式字符，为后续处理提供干净输入。</p>
<h3 id="文本分块模块-split-into-chunks"><a href="#文本分块模块-split-into-chunks" class="headerlink" title="文本分块模块 (split_into_chunks)"></a>文本分块模块 (split_into_chunks)</h3><p>在论文阅读任务中，文献的长度往往很长，tokenize之后很可能超出了大模型的输入限制（或者因为上下文长度限制导致输出内容过少）。因此，笔者先使用<code>cl100k_base</code>模型对论文进行文本分块，再将分块之后的token整合之后传给通义模型。基于<code>tiktoken</code>的<code>cl100k_base</code>编码器实现智能分块，具备以下特性：</p>
<ul>
<li><strong>段落完整性保护</strong>：以自然段落为最小单位，避免分块时拆分逻辑语义。</li>
<li><strong>动态分块策略</strong>：设置8000token的阈值（为API预留空间），对超长段落实施二次分割。</li>
<li><strong>容错机制</strong>：通过正则表达式兼容不同换行符格式，确保分割稳定性。</li>
</ul>
<h3 id="摘要生成模块-summarize-document"><a href="#摘要生成模块-summarize-document" class="headerlink" title="摘要生成模块(summarize_document)"></a>摘要生成模块(summarize_document)</h3><p>该模块整合了分块处理和AI摘要技术：</p>
<ul>
<li><strong>分块摘要</strong>：采用迭代式请求策略，每块生成带层级结构的Markdown摘要。</li>
<li><strong>双重校验机制</strong>：首轮生成后自动检测摘要长度，超限时执行二次精炼。</li>
<li><strong>异常处理</strong>：对API调用失败的分块记录错误日志，保证流程持续运行；同时可根据API调用失败的错误日志查阅阿里官方的大模型产品文档，以寻求支持。</li>
</ul>
<h3 id="格式转换模块-convert-markdown-to-pdf"><a href="#格式转换模块-convert-markdown-to-pdf" class="headerlink" title="格式转换模块(convert_markdown_to_pdf)"></a>格式转换模块(convert_markdown_to_pdf)</h3><p>基于pandoc实现文档格式转换，核心特性包括：</p>
<ul>
<li><strong>中文排版优化</strong>：通过xelatex引擎配置微软雅黑字体，确保中文PDF正确渲染</li>
<li><strong>临时文件管理</strong>：采用写入-转换-清理模式，避免产生冗余文件</li>
<li><strong>路径验证</strong>：自动检测输出目录有效性，异常时触发系统告警</li>
</ul>
<h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> PyPDF2</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_useful_text_from_pdf</span>(<span class="params">pdf_path</span>):</span><br><span class="line">    all_text = []</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(pdf_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            reader = PyPDF2.PdfReader(f)</span><br><span class="line">            <span class="keyword">for</span> page <span class="keyword">in</span> reader.pages:</span><br><span class="line">                text = page.extract_text()</span><br><span class="line">                <span class="keyword">if</span> text:</span><br><span class="line">                    lines = text.splitlines()</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">len</span>(lines) &gt; <span class="number">2</span>:</span><br><span class="line">                        lines = lines[<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">                    page_text = <span class="string">&quot;\n&quot;</span>.join(lines)</span><br><span class="line">                    all_text.append(page_text)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;读取PDF出错：<span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;\n&quot;</span>.join(all_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_into_chunks</span>(<span class="params">text, max_tokens=<span class="number">8000</span></span>):</span><br><span class="line">    tokenizer = tiktoken.get_encoding(<span class="string">&quot;cl100k_base&quot;</span>)</span><br><span class="line"></span><br><span class="line">    paragraphs = re.split(<span class="string">r&#x27;\n\s*\n|\r\n\s*\r\n&#x27;</span>, text.strip())</span><br><span class="line"></span><br><span class="line">    chunks = []</span><br><span class="line">    current_chunk = []</span><br><span class="line">    current_token_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> para <span class="keyword">in</span> paragraphs:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> para.strip():</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        para_tokens = <span class="built_in">len</span>(tokenizer.encode(para))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> para_tokens &gt; max_tokens * <span class="number">0.8</span>:</span><br><span class="line">            sub_paras = re.split(<span class="string">r&#x27;(?&lt;=[。！？；]) +&#x27;</span>, para)</span><br><span class="line">            <span class="keyword">for</span> sub_para <span class="keyword">in</span> sub_paras:</span><br><span class="line">                sub_tokens = <span class="built_in">len</span>(tokenizer.encode(sub_para))</span><br><span class="line">                <span class="keyword">if</span> current_token_count + sub_tokens &gt; max_tokens:</span><br><span class="line">                    chunks.append(<span class="string">&quot;\n\n&quot;</span>.join(current_chunk))</span><br><span class="line">                    current_chunk = [sub_para]</span><br><span class="line">                    current_token_count = sub_tokens</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    current_chunk.append(sub_para)</span><br><span class="line">                    current_token_count += sub_tokens</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> current_token_count + para_tokens &gt; max_tokens:</span><br><span class="line">                chunks.append(<span class="string">&quot;\n\n&quot;</span>.join(current_chunk))</span><br><span class="line">                current_chunk = [para]</span><br><span class="line">                current_token_count = para_tokens</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                current_chunk.append(para)</span><br><span class="line">                current_token_count += para_tokens</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> current_chunk:</span><br><span class="line">        chunks.append(<span class="string">&quot;\n\n&quot;</span>.join(current_chunk))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> chunks</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">summarize_document</span>(<span class="params">text</span>):</span><br><span class="line">    CHUNK_MAX_TOKENS = <span class="number">8000</span>  </span><br><span class="line">    SUMMARY_MAX_TOKENS = <span class="number">8192</span>  </span><br><span class="line"></span><br><span class="line">    chunks = split_into_chunks(text, max_tokens=CHUNK_MAX_TOKENS)</span><br><span class="line"></span><br><span class="line">    all_summaries = []</span><br><span class="line">    client = OpenAI(</span><br><span class="line">        api_key=<span class="string">&quot;YOUR_API_KEY&quot;</span>,</span><br><span class="line">        base_url=<span class="string">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, chunk <span class="keyword">in</span> <span class="built_in">enumerate</span>(chunks):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            prompt = (</span><br><span class="line">                    <span class="string">f&quot;请阅读以下文献的第<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>部分，用中文返回文献框架的markdown源代码，并且每个标题下要把文章中这部分内容做一个简单的概括。 请一定注意：\n&quot;</span></span><br><span class="line">                    <span class="string">&quot;1.除了markdown源代码之外，任何内容都不要输出。\n&quot;</span></span><br><span class="line">                    <span class="string">&quot;2.输出语言必须为中文。【非常重要！】\n&quot;</span></span><br><span class="line">                    <span class="string">&quot;3.用多级的逻辑结构（一级标题、二级标题甚至三级标题）来总结原文的框架，尽可能丰满你的框架\n&quot;</span> + text</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            response = client.chat.completions.create(</span><br><span class="line">                model=<span class="string">&quot;qwen-turbo-2024-11-01&quot;</span>,</span><br><span class="line">                messages=[</span><br><span class="line">                    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">                     <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;你是一个<span class="subst">&#123;area&#125;</span>领域的文献分析助手，擅长从长文档中提取结构化框架并生成技术性摘要。&quot;</span>&#125;,</span><br><span class="line">                    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;</span><br><span class="line">                ],</span><br><span class="line">                temperature=<span class="number">0.4</span>,</span><br><span class="line">                top_p=<span class="number">0.7</span>,</span><br><span class="line">                max_tokens=SUMMARY_MAX_TOKENS,</span><br><span class="line">                frequency_penalty=<span class="number">0.5</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            summary = response.choices[<span class="number">0</span>].message.content.strip()</span><br><span class="line">            all_summaries.append(summary)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;第<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>部分处理失败：<span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">            all_summaries.append(<span class="string">f&quot;## 第<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>部分摘要生成失败\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    final_summary = <span class="string">&quot;\n\n&quot;</span>.join(all_summaries)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(final_summary) &gt; SUMMARY_MAX_TOKENS * <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            response = client.chat.completions.create(</span><br><span class="line">                model=<span class="string">&quot;qwen-turbo-2024-11-01&quot;</span>,</span><br><span class="line">                messages=[</span><br><span class="line">                    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个摘要精炼专家，擅长将多个章节摘要整合成连贯的完整文档摘要&quot;</span>&#125;,</span><br><span class="line">                    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;请将以下分块摘要整合为完整的文献框架，你只能输出一段完整的markdown代码，其他的都不要输出：\n<span class="subst">&#123;final_summary&#125;</span>&quot;</span>&#125;</span><br><span class="line">                ],</span><br><span class="line">                temperature=<span class="number">0.3</span>,</span><br><span class="line">                top_p=<span class="number">0.8</span>,</span><br><span class="line">                max_tokens=SUMMARY_MAX_TOKENS</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">return</span> response.choices[<span class="number">0</span>].message.content.strip()</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">return</span> final_summary  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> final_summary</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_markdown</span>(<span class="params">md_text</span>):</span><br><span class="line">    <span class="keyword">if</span> md_text.startswith(<span class="string">&quot;```&quot;</span>) <span class="keyword">and</span> md_text.rstrip().endswith(<span class="string">&quot;```&quot;</span>):</span><br><span class="line">        lines = md_text.splitlines()</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;\n&quot;</span>.join(lines[<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> md_text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_markdown_to_pdf</span>(<span class="params">markdown_text, output_path</span>):</span><br><span class="line">    temp_md_path = <span class="string">r&quot;D:\TestForEssayReader\Temp\temp.md&quot;</span></span><br><span class="line"></span><br><span class="line">    markdown_text = clean_markdown(markdown_text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(temp_md_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(markdown_text)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;临时Markdown文件已创建: <span class="subst">&#123;os.path.abspath(temp_md_path)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        subprocess.run(</span><br><span class="line">            [</span><br><span class="line">                pandoc,</span><br><span class="line">                temp_md_path,</span><br><span class="line">                <span class="string">&quot;-f&quot;</span>, <span class="string">&quot;markdown&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-o&quot;</span>, output_path,</span><br><span class="line">                <span class="string">&quot;--pdf-engine=xelatex&quot;</span>,  </span><br><span class="line">                <span class="string">&quot;-V&quot;</span>, <span class="string">&quot;CJKmainfont=Microsoft YaHei&quot;</span> </span><br><span class="line">            ],</span><br><span class="line">            check=<span class="literal">True</span>,</span><br><span class="line">            stdout=subprocess.PIPE,</span><br><span class="line">            stderr=subprocess.PIPE,</span><br><span class="line">            text=<span class="literal">True</span>,</span><br><span class="line">            shell=<span class="literal">True</span>, </span><br><span class="line">            encoding = <span class="string">&#x27;utf-8&#x27;</span>,  </span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;PDF成功生成于：<span class="subst">&#123;os.path.abspath(output_path)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> subprocess.CalledProcessError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;转换失败，错误详情：\n<span class="subst">&#123;e.stderr&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;操作出错：<span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        <span class="keyword">if</span> os.path.exists(temp_md_path):</span><br><span class="line">            os.remove(temp_md_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_pandoc</span>():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        subprocess.run([<span class="string">&quot;pandoc&quot;</span>, <span class="string">&quot;--version&quot;</span>], check=<span class="literal">True</span>, stdout=subprocess.PIPE)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;pandoc&quot;</span></span><br><span class="line">    <span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">        common_paths = [</span><br><span class="line">            <span class="string">r&quot;C:\Program Files\Pandoc\pandoc.exe&quot;</span>,</span><br><span class="line">            <span class="string">r&quot;C:\Users\&#123;&#125;\AppData\Local\Pandoc\pandoc.exe&quot;</span>.<span class="built_in">format</span>(os.getenv(<span class="string">&quot;USERNAME&quot;</span>))</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> path <span class="keyword">in</span> common_paths:</span><br><span class="line">            <span class="keyword">if</span> os.path.exists(path):</span><br><span class="line">                <span class="keyword">return</span> path</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    area = <span class="built_in">input</span>(<span class="string">&quot;请输入你的专业：&quot;</span>)</span><br><span class="line">    YOUR_API_KEY = <span class="built_in">input</span>(<span class="string">&#x27;请输入你的API_KEY(请在“ https://bailian.console.aliyun.com/?apiKey=1#/api-key ” 申请阿里API)：&#x27;</span>)</span><br><span class="line">    essay_folder = <span class="built_in">input</span>(<span class="string">&quot;请输入你原始文献文件夹的路径：&quot;</span>)</span><br><span class="line">    summary_folder = <span class="built_in">input</span>(<span class="string">&quot;请输入你希望保存摘要的文件夹路径：&quot;</span>)</span><br><span class="line"></span><br><span class="line">    os.makedirs(summary_folder, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(essay_folder):</span><br><span class="line">        <span class="keyword">if</span> filename.lower().endswith(<span class="string">&quot;.pdf&quot;</span>):</span><br><span class="line">            pdf_path = os.path.join(essay_folder, filename)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;正在处理：<span class="subst">&#123;pdf_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            text = extract_useful_text_from_pdf(pdf_path)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> text.strip():</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;未提取到有效文本，跳过该文件。&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            markdown_summary = summarize_document(text)</span><br><span class="line"></span><br><span class="line">            output_pdf = os.path.join(summary_folder, <span class="string">f&quot;summary_<span class="subst">&#123;os.path.splitext(filename)[<span class="number">0</span>]&#125;</span>.pdf&quot;</span>)</span><br><span class="line">            convert_markdown_to_pdf(markdown_summary, output_pdf)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;生成摘要文件：<span class="subst">&#123;output_pdf&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h2><ul>
<li>请安装<code>python</code>环境，并安装<code>os</code>,<code>tiktoken</code>,<code>re</code>,<code>openai</code>,<code>subprocess</code>,<code>PyPDF2</code>软件包。</li>
<li>请安装<code>pandoc</code>，并配置系统环境变量</li>
<li>请申请阿里的API，网址：<a target="_blank" rel="noopener" href="https://bailian.console.aliyun.com/?apiKey=1#/api-key">https://bailian.console.aliyun.com/?apiKey=1#/api-key</a></li>
<li>请创建一个文件夹，其中存放您要处理的文献原始文件（pdf格式）</li>
</ul>
<h2 id="输出结果示例"><a href="#输出结果示例" class="headerlink" title="输出结果示例"></a>输出结果示例</h2><p>在<code>python</code>终端，可看到程序运行结果如下：<br><img src="/images/image-1.png" alt="alt text"><br>在设定的概要输出路径下，可看到概要文件如下：<br><img src="/images/image-3.png" alt="alt text"><br>点开后内容如下，层次分明、结构清晰，便于科研工作者迅速了解文献内容。<br><img src="/images/image-2.png" alt="alt text"></p>
<h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><p>本项目是笔者在学习了一点大模型技术后，利用半天时间开发的小型应用。在使用过程中，可能会在整合多个chunk的时候出现格式问题，导致生成的PDF中保留了一部分Markdown源码，这个问题会慢慢解决。当然也可能会有其他的问题，笔者尚未发现。<br>这篇文档会先在笔者的个人博客上发布，稍后会整理并发布在<a target="_blank" rel="noopener" href="https://github.com/Dasai-Hzm">我的Github</a>上，欢迎读者在自己科研时使用这个小程序，通过Github社区与我交流。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dasai-hzm.github.io/2025/01/19/summary-2024-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="HzmSailor">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HzmSailor's Learning Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/19/summary-2024-1/" class="post-title-link" itemprop="url">大一上学期总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-01-19 22:36:16 / 修改时间：23:30:02" itemprop="dateCreated datePublished" datetime="2025-01-19T22:36:16+08:00">2025-01-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%97%A5%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">日记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p style="text-indent: 2em;">期末成绩陆陆续续出来得差不多了，我也终于找到时间浅浅总结一下这学期的收获了。</p>

<p style="text-indent: 2em;">刚从高中模式中逃脱出来的人大概是逃不开高中的思维，身边有不少紧紧盯着成绩的朋友（当然我也算是，毕竟是第一次大学的考试，陋习难改）。若是但从分数上看，除了线性代数寄的很惨之外，其他倒也能接受。充满智慧的教务老师们，为了“降低压力和内卷”，把成绩分布和排位功能给取消了，我也尚不清楚我这均分是个什么水平。</p>

<p style="text-indent: 2em;">只不过，若但从期末考试的卷子上看这一学期的收获，也未免过于肤浅了，毕竟如果要评论一下这学期的一些课：</p>
<p style="text-indent: 2em;">（1）思政、国防、毛概，这三门是每门一天速通的。据此总结了经验：摆烂一学期，考前速通也能90+（当然更高我也不追求）。</p>
<p style="text-indent: 2em;">（2）高数和线代的内容，大部分高中竞赛时期就已经掌握得很熟练了，而且XJTU考试题的难度相比华科实在是简单，再往上跟USTC、PKU、THU更没法比。</p>
<p style="text-indent: 2em;">（3）婴语，主要是学术写作的内容，学到了很多学术写作的格式和规范，收获很多，也很喜欢楚老师。但其实一周一次两小时的婴语课，也提高不了多少真正的英语水平，考个四级就老实了。</p>
<p style="text-indent: 2em;">（4）选修课更是水的一批，这学期选了一门生态学、一门材料学的课。感觉选这些“推荐的好课”确实是上课水、不点名、给分高，但是1门课就要白白浪费32个小时（毕竟上课写作业也没啥效率，如果不逃课，那课堂的时间确实很浪费）。上完之后有一种强烈的空虚感，应该再也不会选这种课了。</p>

<p style="text-indent: 2em;">那这学期到底收获了什么呢？</p>

<p style="text-indent: 2em;">第一，学会了编程，算是小白入门的第一课。编程从0学起，而且基本上是让GPT教我、帮我debug，靠着PTA上的作业题学会的。这让我深刻体会到了“大不了自学”的含义。作为高考选拔进入，曾经有自认为不错的物理竞赛背景的同学，我选择CS类的专业，本身并没有什么基础优势，编程都是现学的，算法更是一窍不通。这学期从科学上网、学会github、再到学习一部分pytorch、搭建个人博客，虽然水平远比不上少班的大佬们，但总算是学习了一点点技术，这应该是未来工作/研究中比较实用的部分。</p>

<p style="text-indent: 2em;">第二，认识了很多新朋友，社交扩大了一些。很不幸，高中同学没有来XJTU的，以前认识的朋友进入XJTU的也不多，再加上我是后选拔进班，和其他同学军训和住宿都不在一起，social十分困难。我也得强行转变一下MBTI，变得E一些，主动加了不少朋友，也很有幸能认识到很多少班的大佬们。非常感谢大佬们不吝赐教，这学期给了我很多宝贵的指导和帮助。</p>

<p style="text-indent: 2em;">第三，AI学组运行平稳起步，顺利走上正轨。当时本着想为同学们做些事情的想法，再加上自己时间比较宽裕，就接下了24级学组负责人的职位。我接下工作后才发现，学组各方面制度和架构其实很不完善，创立时间也不长。怎么把学组建设好、运营好，让学组中各位大佬团结起来、发挥聪明才智、创造集体价值，是摆在我面前的一个重要问题。从一开始准备见面会，分配工作，到后来一项一项推进工作，产出一份一份资料，学组的每一项工作都倾注着我们的付出，凝聚着所有人的努力，我也当然为此投入了大量的时间和精力。</p>

<p style="text-indent: 2em;">第四，身体素质相比有了很大提高。荒废了半年多的长跑慢慢拾起，从一开始的两公里六分配，用两个月的时间一点一点提高到五公里四分半的水平。在无数个零下黑夜的田径场上，音乐、脚步和呼吸共同律动，我似乎又找回五年前那个1500米银牌的自己了。相比去年那个一周跑几趟医院的我，这学期去医院的次数明显少了很多（或许也是因为大部分问题都能自己诊断然后吃药解决）。然而不幸的是，期末周的来临，让我有半个多月找不到跑步的时间，连续一天一门速通的压力也让我身体扛不住了，期末考完一回家就直接急诊科输液去了。</p>

<p style="text-indent: 2em;">看起来很棒，所以下学期有什么打算呢？</p>

<p style="text-indent: 2em;">这个问题我想了很久，因为计划的制定是从最核心的价值观出发的。所以，这必须要回答这样的问题：我看重什么呢？这学期最后一次心理咨询的时候，老师抛给我一个这样的问题，她希望我寒假的时候好好思考一下。</p>

<p style="text-indent: 2em;">我追求的是90分提高到95分，然后获得4.3的绩点的那种满足吗？还是门门课都要争取上90，最后拿奖学金？如果为了成绩/科研天天晚上学到1点以后睡觉，身体素质下降，这是我想要的吗？</p>

<p style="text-indent: 2em;">首先，我想我来XJTU是为了学知识和技术的，上课的主要意义是学知识，成绩只是学习成果的一个反映，大概比保研线高一些，维持在安全能保研的水平就够了，我认为自己没有能力也没有心力去争前几名。既然上课的主要意义是学知识，那么我宁愿选一些上课水平高但是给分低的老师，或是内容我感兴趣/内容有价值但给分低的课，而不会去选那些给分高但是上完之后没什么收获的课。当然，一些必修课如果像后者一样，不如翘之，然后从网上找替代品学习。</p>

<p style="text-indent: 2em;">至于奖学金，我倒认为争取的必要性不大，可以随缘处之，有机会就拿，拿不成就算了。如果为了这几千块钱牺牲掉一些更为重要的价值，那真是捡了芝麻丢了西瓜，没有抓住主要矛盾。</p>

<p style="text-indent: 2em;">其次，身体素质上我希望能继续提升一些。跑半马和爬雪山是我一直以来的两个梦想，希望下个学期能至少实现一个。跑半马需要月跑量100km以上，至少训练半年，注重10km/15km这样的长距离跑步训练（这学期的5km训练是远远不够的）。爬雪山也需要半年以上的准备时间，长跑训练、负重爬楼训练、肌肉耐力训练都要加上，而且还需要一些训练营培训一些冰雪技巧（不过这倒不难，身体素质是最硬的一道关）。另外，规律的作息也是身体素质的一个保障。希望下个学期还能保证12点半以前入睡，然后8点左右起来的充足睡眠（听起来不像是上大学的，但我这个学期确实如此）。身体和事业是同等重要的，如果工作真的需要我熬夜处理，那我认为应该放弃一部分工作来保证睡眠。</p>

<p style="text-indent: 2em;">还有科研的问题，身边有不少大一就进组的同学，但是盲从跟风并不是我想要的，我更希望的是能有机会投入到自己真正感兴趣的方向中去。毕竟有兴趣才会有动力，工作起来有激情就不会感到疲惫，若是单纯以论文成果为导向，恐怕我没法承受这种强度的枯燥工作。另外，目前自己的水平也十分有限，提高能力是根本，基础知识应该格外重视，不能急于进组科研，更不能急于追求成果。</p>

<p style="text-indent: 2em;">最后，希望学组的工作能继续推进一些，希望能推动更多同学加入到资料编写、学业互助的工作中来，让学组创造更大的社会价值，也在学组的工作中不断积累经验，提高自己的专业能力、管理能力。</p>

<p style="text-indent: 2em;">这学期的故事就到这里了，下个学期会更好，一定会。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="HzmSailor"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">HzmSailor</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Dasai-Hzm" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Dasai-Hzm" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zimuhan276@gmail.com" title="E-Mail → mailto:zimuhan276@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://xjtu-ai.github.io/" title="http:&#x2F;&#x2F;xjtu-ai.github.io" rel="noopener" target="_blank">XJTU-AI学组</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HzmSailor</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '[object Object]';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
